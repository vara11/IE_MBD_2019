{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adiletgaparov/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from category_encoders import CountEncoder # requires separate installation\n",
    "from custom_transformers import OurSimpleImputer, OurAdvancedImputer, DataCorrection, DropColumns\n",
    "from custom_transformers import GeoClustering, Distance, Interactions, OtherFeatures\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_values = pd.read_csv('data/train.csv')\n",
    "original_train_labels = pd.read_csv('data/train_labels.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "original_train = pd.merge(original_train_values, original_train_labels, on='id')\n",
    "original_train.date_recorded = pd.to_datetime(original_train.date_recorded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = original_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. data understanding / preparation / cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 meaningless features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are few columns that contain the same information but on different granularity level. In each case, we decided to keep columns that have more granular data and in case we needed to group them later, we would be able to do that. Columns in bold are those that we keep:\n",
    "1. <strong>waterpoint_type</strong> / waterpoint_type_group\n",
    "2. <strong>source</strong> / source_type / source_class\n",
    "3. <strong>quantity</strong> / quantity_group\n",
    "4. <strong>water_quality</strong> / quality_group\n",
    "5. <strong>payment</strong> / payment_type\n",
    "6. <strong>management</strong> / management_group\n",
    "7. <strong>extraction_type</strong> / extraction_type_group / extraction_type_class\n",
    "8. <strong>scheme_management</strong> / scheme_name\n",
    "----- \n",
    "##### Besides, we decided to drop the following features because they don't contain useful information:\n",
    "1. <strong>recorded_by</strong>: contains only one value, which is organization that recorded the data\n",
    "2. <strong>id</strong>: surrogate key, column representing id of the observation in numerical format, doesn't contain any information.\n",
    "----- \n",
    "##### We also decided to drop 4 features related to location (region, district_code, region_code, subvillage) for the following reasons:\n",
    "1. combination of the features <strong>district_code, region_code</strong> gives us zip code which identifies the location, but we can get the same information in more detailed way from geograhic coordinates (<strong>longitude, latitude</strong>)\n",
    "2. we have feature <strong>lga</strong> that contains similar information to <strong>region_code, region</strong> but on more granular level\n",
    "3. we will use <strong>latitude</strong> and <strong>longitude</strong> for creating clusters of regions with more granularity, therefore, we can drop <strong>subvillage</strong> too\n",
    "----- \n",
    "##### We are dropping <strong>public_meeting</strong> too for two reasons:\n",
    "1. 86% of values are the same\n",
    "2. According to research on water issue in Tanzania, public meeting at installation is not good predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['id','waterpoint_type_group', 'source_type', 'source_class', \n",
    "                   'quantity_group', 'quality_group', 'payment_type', \n",
    "                   'management_group', 'extraction_type_group', 'extraction_type_class', \n",
    "                   'scheme_name', 'recorded_by', 'district_code', \n",
    "                   'region_code', 'region','subvillage', 'public_meeting']\n",
    "\n",
    "train = train.drop(columns_to_drop, axis=1)\n",
    "test = test.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features with various names for missing value\n",
    "The dataset is quite dirty in a sense that missing values are represented in various forms:\n",
    "1. simply NaN / missing cells\n",
    "2. contains value \"unknown\", \"Not known\", \"not known\", \"-\", \"No\", \"no\", \"0\", \"unknown\", \"none\".\n",
    "\n",
    "We tried to find those values that might represent missing value in each column and standardize them by replacing all those columns with \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = OurSimpleImputer(coords=False).fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### longitude and latitude\n",
    "After checking longitude and latitude, we found that we have some of the values to be 0. Given that this is Tanzania, latitude and longitude of 0 or close to 0 are simply impossible. We decided to replace those values with mean for each LGA, which has quite detailed granularity (125 LGAs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = OurSimpleImputer(categorical=False).fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### population\n",
    "Almost 48% of observations have value \"population\" equal to 0 or 1. According to the description, this feature indicates the population around the particular waterpoint. It is hard to believe that the waterpoint was built for 0 or 1 person. Hence, we believe these two values indicate missing values or incorrect recording. We initially wanted to take population from other dataset and merge it with this one, however, the definition of population feature in this dataset is slightly different. Given that this is a population <strong>around</strong> a particular waterpoint, we decided to use population average of each coordinates-based cluster to impute missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Seems like 50 is good number of K-Means clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = train[['latitude','longitude']]\n",
    "locations_transformed = StandardScaler().fit_transform(locations)\n",
    "\n",
    "n_clusters = [10, 20, 30, 40, 60, 80, 100, 150]\n",
    "inertia = []\n",
    "\n",
    "for num in n_clusters:\n",
    "    kmeans_model = KMeans(n_clusters=num, n_jobs=-1).fit(locations_transformed)\n",
    "    wss = kmeans_model.inertia_\n",
    "    inertia.append(wss)\n",
    "    \n",
    "pd.DataFrame({'n_clusters':n_clusters, 'inertia': inertia}).plot(x='n_clusters', y='inertia');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Clustering().fit_transform()\n",
    "train = OurAdvancedImputer().fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 data correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installer and Funder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataCorrection(installer=True, funder=True).fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "1. Clustering based on lat/lon\n",
    "2. Distance to centers (manhattan and eucledian)\n",
    "3. Bucketization for population\n",
    "4. Interactions\n",
    "    1. scheme_management + payment\n",
    "    2. basin + source\n",
    "    3. source + waterpoint_type\n",
    "    4. extraction_type + waterpoint_type\n",
    "    5. source + extraction_type\n",
    "    6. water_quality + quantity\n",
    "    7. extaction_type + payment\n",
    "5. Dry season from date_recorded (True/False)\n",
    "6. Replace num_private to True/False\n",
    "7. CountEncoder only: funder, installer, wpt_name, lga, ward\n",
    "8. 1Hot / CountEncoder: basin(9), scheme_management(13), management(12), source(10), payment(7), extraction_type(18), waterpoint_type(7), water_quality(8), quantity(5)\n",
    "9. Age feature: year the data was recorded - construction year\n",
    "10. Construction year and amount_tsh?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningless_cols = ['id','waterpoint_type_group', 'source_type', 'source_class', \n",
    "                   'quantity_group', 'quality_group', 'payment_type', \n",
    "                   'management_group', 'extraction_type_group', 'extraction_type_class', \n",
    "                   'scheme_name', 'recorded_by', 'district_code', \n",
    "                   'region_code', 'region','subvillage', 'public_meeting']\n",
    "\n",
    "features_to_drop = ['latitude', 'longitude', 'date_recorded', 'num_private']\n",
    "\n",
    "transformation_pipeline = Pipeline([\n",
    "    ('meaningless_features', DropColumns(meaningless_cols)),\n",
    "    ('simple_imputer', OurSimpleImputer(permit=False)),\n",
    "    ('government', DataCorrection(installer=True, funder=True)),\n",
    "    ('geo_clusters', GeoClustering()),\n",
    "    ('advanced_imputer', OurAdvancedImputer(population_bucket=False)),\n",
    "    ('distance', Distance()),\n",
    "    ('interactions', Interactions()),\n",
    "    ('other_features', OtherFeatures(num_private=False)),\n",
    "    ('drop', DropColumns(features_to_drop))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prep = transformation_pipeline.fit_transform(original_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(df, feature, target):\n",
    "    p_target = df[target].value_counts(normalize=True)\n",
    "    H_target = np.nansum(-np.log2(p_target) * p_target)\n",
    "    \n",
    "    p_feature_target = df.groupby(feature)[target].value_counts(normalize=True)\n",
    "    p_features = df[feature].value_counts(normalize=True)\n",
    "    \n",
    "    H_feature = 0\n",
    "    for value in df[feature].unique():\n",
    "        H_value = p_features[value] * np.nansum(-np.log2(p_feature_target[value]) * p_feature_target[value])\n",
    "        H_feature += H_value\n",
    "    \n",
    "    return H_target - H_feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_prep.columns.drop('status_group')\n",
    "information_gain_features = {}\n",
    "for feature in features:\n",
    "    information_gain_features[feature] = information_gain(train_prep, feature, 'status_group')\n",
    "sorted(information_gain_features.items(), key=lambda tup: -tup[1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "1. permit: if NA are imputed with True, information gain is 0.00083. If with False, then 0.00075. We are dropping this feature.\n",
    "2. num_private: very low value of 0.00016, we decided to drop this feature\n",
    "3. population_binned: very low value of 0.00226, we decided not to create this feature in next iteration and pipeline\n",
    "4. installer: not corrected version of installer gives 0.189, corrected/grouped version gives 0.143\n",
    "5. funder: not corrected version of installer gives 0.176, corrected/grouped version gives 0.148\n",
    "6. cluster: k=50 clusters gives 0.084\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 LogisticRegression L1 selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_prep[train_prep.columns.drop(\"status_group\")], train['status_group']\n",
    "\n",
    "transformation_pipeline = Pipeline([\n",
    "    ('count_encoder', CountEncoder(handle_unknown=0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X = transformation_pipeline.fit_transform(X)\n",
    "\n",
    "logistic = LogisticRegressionCV(Cs=[3, 10, 30, 100], penalty=\"l1\", \n",
    "                                multi_class='multinomial',solver='saga', cv=3, n_jobs=-1).fit(X, y);\n",
    "model = SelectFromModel(logistic, prefit=True);\n",
    "\n",
    "X_new = model.transform(X);\n",
    "\n",
    "selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                 index=train_prep.index,\n",
    "                                 columns=train_prep.drop('status_group', axis=1).columns)\n",
    "\n",
    "dropped_columns = selected_features.columns[selected_features.var() == 0]\n",
    "dropped_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "Lasso selection didn't remove any features, so we can continue with modeling and hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. modeling and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Random Forest without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningless_cols = ['id','waterpoint_type_group', 'source_type', 'source_class', \n",
    "                   'quantity_group', 'quality_group', 'payment_type', \n",
    "                   'management_group', 'extraction_type_group', 'extraction_type_class', \n",
    "                   'scheme_name', 'recorded_by', 'district_code', \n",
    "                   'region_code', 'region','subvillage', 'public_meeting']\n",
    "\n",
    "features_to_drop = ['latitude', 'longitude', 'date_recorded', 'num_private', 'permit', 'water_quality']\n",
    "\n",
    "transformation_pipeline = Pipeline([\n",
    "    ('meaningless_features', DropColumns(meaningless_cols)),\n",
    "    ('simple_imputer', OurSimpleImputer(permit=False)),\n",
    "    ('government', DataCorrection(installer=False, funder=False)),\n",
    "    ('geo_clusters', GeoClustering()),\n",
    "    ('advanced_imputer', OurAdvancedImputer(population_bucket=False)),\n",
    "    ('distance', Distance(strategy='eucledian')),\n",
    "    ('interactions', Interactions()),\n",
    "    ('other_features', OtherFeatures(num_private=False, dry_season=False)),\n",
    "    ('drop', DropColumns(features_to_drop)),\n",
    "    ('encoder', CountEncoder(handle_unknown=0))\n",
    "])\n",
    "\n",
    "class_to_num = {'functional': 2, 'non functional': 0, 'functional needs repair': 1}\n",
    "num_to_class = {0:'non functional', 1: 'functional needs repair', 2: 'functional'} \n",
    "\n",
    "X = original_train.drop('status_group', axis=1)\n",
    "y = original_train.status_group.replace(class_to_num)\n",
    "\n",
    "X_prep = transformation_pipeline.fit_transform(X)\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# setting grid\n",
    "param_grid_forest = {\n",
    "    'max_depth': [10, 25, 50, None],\n",
    "    'n_estimators': [50, 100]\n",
    "}\n",
    "\n",
    "grid_search_forest = GridSearchCV(rf, cv=5, param_grid=param_grid_forest,n_jobs=-1).fit(X_prep, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train performance on Random Forest w/o PCA: ', grid_search_forest.best_estimator_.score(X_prep, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['rank_test_score','mean_test_score', 'param_max_depth', 'param_n_estimators']\n",
    "pd.DataFrame(grid_search_forest.cv_results_)[params].sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([*zip(X_prep.columns, grid_search_forest.best_estimator_.feature_importances_)], key=lambda tup: -tup[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error analysis\n",
    "\n",
    "* Rows: actual class\n",
    "* Columns: predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv = cross_val_predict(grid_search_forest.best_estimator_, X_prep, y, cv=5)\n",
    "conf_mx = confusion_matrix(y, y_pred_cv)\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True) \n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "Our model is overfitting based on the difference between train performance (0.99) and cross-validation performance (0.81). \n",
    "Feature importance shows that wee need to drop two features \"dry_season\" and \"water_quality\". After removing those two features and reducing max_depth to 100, our generalization didn't improve. We figured out also that using \"Eucledian\" distance gives slightly better performance than \"Manhattan\" in transformer \"Distance()\". \n",
    "\n",
    "Based on error analysis, we can see that Random Forest is doing a lot of mistakes in predicting class 1 which is \"non functional needs repair\". This is understable given the heavy imbalance of this class (around 7% of classes). \n",
    "\n",
    "The best parameters based on CV: \n",
    "1. n_estimators = 100\n",
    "2. max_depth = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep = transformation_pipeline.transform(test)\n",
    "y_pred = grid_search_forest.best_estimator_.predict(test_prep)\n",
    "\n",
    "test_id = pd.read_csv('data/test.csv').id\n",
    "submission = pd.DataFrame({\"id\": test_id, \"status_group\": y_pred})\n",
    "submission = submission.replace({'status_group': num_to_class})\n",
    "submission.to_csv('submission_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Random Forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningless_cols = ['id','waterpoint_type_group', 'source_type', 'source_class', \n",
    "                   'quantity_group', 'quality_group', 'payment_type', \n",
    "                   'management_group', 'extraction_type_group', 'extraction_type_class', \n",
    "                   'scheme_name', 'recorded_by', 'district_code', \n",
    "                   'region_code', 'region','subvillage', 'public_meeting']\n",
    "\n",
    "features_to_drop = ['latitude', 'longitude', 'date_recorded', 'num_private', 'permit', 'water_quality']\n",
    "\n",
    "transformation_pipeline = Pipeline([\n",
    "    ('meaningless_features', DropColumns(meaningless_cols)),\n",
    "    ('simple_imputer', OurSimpleImputer(permit=False)),\n",
    "    ('government', DataCorrection(installer=False, funder=False)),\n",
    "    ('geo_clusters', GeoClustering()),\n",
    "    ('advanced_imputer', OurAdvancedImputer(population_bucket=False)),\n",
    "    ('distance', Distance(strategy='eucledian')),\n",
    "    ('interactions', Interactions()),\n",
    "    ('other_features', OtherFeatures(num_private=False, dry_season=False)),\n",
    "    ('drop', DropColumns(features_to_drop)),\n",
    "    ('encoder', CountEncoder(handle_unknown=0)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('boxcos', PowerTransformer()),\n",
    "    ('PCA', PCA())\n",
    "])\n",
    "\n",
    "class_to_num = {'functional': 2, 'non functional': 0, 'functional needs repair': 1}\n",
    "num_to_class = {0:'non functional', 1: 'functional needs repair', 2: 'functional'} \n",
    "\n",
    "X = original_train.drop('status_group', axis=1)\n",
    "y = original_train.status_group.replace(class_to_num)\n",
    "\n",
    "X_prep = transformation_pipeline.fit_transform(X)\n",
    "\n",
    "rf_pca = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# setting grid\n",
    "param_grid_forest = {\n",
    "    'max_depth': [10, 25, 50, None],\n",
    "    'n_estimators': [50, 100]\n",
    "}\n",
    "\n",
    "grid_search_forest_pca = GridSearchCV(rf_pca, cv=5, param_grid=param_grid_forest,n_jobs=-1).fit(X_prep, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train performance on Random Forest w/ PCA: ', grid_search_forest_pca.best_estimator_.score(X_prep, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['rank_test_score','mean_test_score', 'param_max_depth', 'param_n_estimators']\n",
    "pd.DataFrame(grid_search_forest_pca.cv_results_)[params].sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error analysis\n",
    "\n",
    "* Rows: actual class\n",
    "* Columns: predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv = cross_val_predict(grid_search_forest_pca.best_estimator_, X_prep, y, cv=5)\n",
    "conf_mx = confusion_matrix(y, y_pred_cv)\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True) \n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "Similar issues with Random Forest run without PCA, except for the performance on cross-validation got worse (from 0.81 to 0.77).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningless_cols = ['id','waterpoint_type_group', 'source_type', 'source_class', \n",
    "                   'quantity_group', 'quality_group', 'payment_type', \n",
    "                   'management_group', 'extraction_type_group', 'extraction_type_class', \n",
    "                   'scheme_name', 'recorded_by', 'district_code', \n",
    "                   'region_code', 'region','subvillage', 'public_meeting']\n",
    "\n",
    "features_to_drop = ['latitude', 'longitude', 'date_recorded', 'num_private', 'permit', 'water_quality']\n",
    "\n",
    "transformation_pipeline = Pipeline([\n",
    "    ('meaningless_features', DropColumns(meaningless_cols)),\n",
    "    ('simple_imputer', OurSimpleImputer(permit=False)),\n",
    "    ('government', DataCorrection(installer=False, funder=False)),\n",
    "    ('geo_clusters', GeoClustering()),\n",
    "    ('advanced_imputer', OurAdvancedImputer(population_bucket=False)),\n",
    "    ('distance', Distance(strategy='eucledian')),\n",
    "    ('interactions', Interactions()),\n",
    "    ('other_features', OtherFeatures(num_private=False, dry_season=False)),\n",
    "    ('drop', DropColumns(features_to_drop)),\n",
    "    ('encoder', CountEncoder(handle_unknown=0))\n",
    "])\n",
    "\n",
    "class_to_num = {'functional': 2, 'non functional': 0, 'functional needs repair': 1}\n",
    "num_to_class = {0:'non functional', 1: 'functional needs repair', 2: 'functional'} \n",
    "\n",
    "X = original_train.drop('status_group', axis=1)\n",
    "y = original_train.status_group.replace(class_to_num)\n",
    "\n",
    "X_prep = transformation_pipeline.fit_transform(X)\n",
    "\n",
    "\n",
    "xgb_model = XGBClassifier(n_jobs=-1)\n",
    "\n",
    "# setting grid\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [20],\n",
    "    'learning_rate': [0.03, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(xgb_model, cv=5, param_grid=param_grid_xgb, n_jobs=-1).fit(X_prep, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train performance on XGB: ', grid_search_xgb.best_estimator_.score(X_prep, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['rank_test_score','mean_test_score', 'param_max_depth', 'param_learning_rate']\n",
    "pd.DataFrame(grid_search_xgb.cv_results_)[params].sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv = cross_val_predict(grid_search_xgb.best_estimator_, X_prep, y, cv=5)\n",
    "conf_mx = confusion_matrix(y, y_pred_cv)\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True) \n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "The best parameters based on CV: \n",
    "1. learning_rate = 0.1\n",
    "2. max_depth = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep = transformation_pipeline.transform(test)\n",
    "y_pred = grid_search_xgb.best_estimator_.predict(test_prep)\n",
    "\n",
    "test_id = pd.read_csv('data/test.csv').id\n",
    "submission = pd.DataFrame({\"id\": test_id, \"status_group\": y_pred})\n",
    "submission = submission.replace({'status_group': num_to_class})\n",
    "submission.to_csv('submission_gbm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 LDA Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningless_cols = ['id','waterpoint_type_group', 'source_type', 'source_class', \n",
    "                   'quantity_group', 'quality_group', 'payment_type', \n",
    "                   'management_group', 'extraction_type_group', 'extraction_type_class', \n",
    "                   'scheme_name', 'recorded_by', 'district_code', \n",
    "                   'region_code', 'region','subvillage', 'public_meeting']\n",
    "\n",
    "features_to_drop = ['latitude', 'longitude', 'date_recorded', 'num_private', 'permit', 'water_quality']\n",
    "\n",
    "transformation_pipeline = Pipeline([\n",
    "    ('meaningless_features', DropColumns(meaningless_cols)),\n",
    "    ('simple_imputer', OurSimpleImputer(permit=False)),\n",
    "    ('government', DataCorrection(installer=False, funder=False)),\n",
    "    ('geo_clusters', GeoClustering()),\n",
    "    ('advanced_imputer', OurAdvancedImputer(population_bucket=False)),\n",
    "    ('distance', Distance(strategy='eucledian')),\n",
    "    ('interactions', Interactions()),\n",
    "    ('other_features', OtherFeatures(num_private=False, dry_season=False)),\n",
    "    ('drop', DropColumns(features_to_drop)),\n",
    "    ('encoder', CountEncoder(handle_unknown=0)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('boxcos', PowerTransformer())\n",
    "])\n",
    "\n",
    "class_to_num = {'functional': 2, 'non functional': 0, 'functional needs repair': 1}\n",
    "num_to_class = {0:'non functional', 1: 'functional needs repair', 2: 'functional'} \n",
    "\n",
    "X = original_train.drop('status_group', axis=1)\n",
    "y = original_train.status_group.replace(class_to_num)\n",
    "\n",
    "X_prep = transformation_pipeline.fit_transform(X)\n",
    "\n",
    "lda = LDA()\n",
    "\n",
    "# setting grid\n",
    "param_grid_lda = {\n",
    "    'n_components': [2, 10]\n",
    "}\n",
    "\n",
    "grid_search_forest_LDA = GridSearchCV(lda, cv=5, param_grid=param_grid_lda,n_jobs=-1).fit(X_prep, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train performance on LDA: ', grid_search_forest_LDA.best_estimator_.score(X_prep, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['rank_test_score','mean_test_score']\n",
    "pd.DataFrame(grid_search_forest_LDA.cv_results_)[params].sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningless_cols = ['id','waterpoint_type_group', 'source_type', 'source_class', \n",
    "                   'quantity_group', 'quality_group', 'payment_type', \n",
    "                   'management_group', 'extraction_type_group', 'extraction_type_class', \n",
    "                   'scheme_name', 'recorded_by', 'district_code', \n",
    "                   'region_code', 'region','subvillage', 'public_meeting']\n",
    "\n",
    "features_to_drop = ['latitude', 'longitude', 'date_recorded', 'num_private', 'permit', 'water_quality']\n",
    "\n",
    "transformation_pipeline = Pipeline([\n",
    "    ('meaningless_features', DropColumns(meaningless_cols)),\n",
    "    ('simple_imputer', OurSimpleImputer(permit=False)),\n",
    "    ('government', DataCorrection(installer=False, funder=False)),\n",
    "    ('geo_clusters', GeoClustering()),\n",
    "    ('advanced_imputer', OurAdvancedImputer(population_bucket=False)),\n",
    "    ('distance', Distance(strategy='eucledian')),\n",
    "    ('interactions', Interactions()),\n",
    "    ('other_features', OtherFeatures(num_private=False, dry_season=False)),\n",
    "    ('drop', DropColumns(features_to_drop)),\n",
    "    ('encoder', CountEncoder(handle_unknown=0))\n",
    "])\n",
    "\n",
    "class_to_num = {'functional': 2, 'non functional': 0, 'functional needs repair': 1}\n",
    "num_to_class = {0:'non functional', 1: 'functional needs repair', 2: 'functional'} \n",
    "\n",
    "X = original_train.drop('status_group', axis=1)\n",
    "y = original_train.status_group.replace(class_to_num)\n",
    "\n",
    "X_prep = transformation_pipeline.fit_transform(X)\n",
    "\n",
    "lgb = LGBMClassifier(objective='multiclass',num_class=3)\n",
    "\n",
    "param_grid_lgb = {\n",
    "    'num_leaves': [64,128],\n",
    "    'learning_rate': [0.3, 1],\n",
    "    'max_depth': [16],\n",
    "    'min_data_in_leaf': [200, 500]\n",
    "}\n",
    "\n",
    "grid_search_lgb = GridSearchCV(lgb, cv=5, param_grid=param_grid_lgb,n_jobs=-1).fit(X_prep, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train performance on LightGBM:  0.8932154882154882\n"
     ]
    }
   ],
   "source": [
    "print('Train performance on LightGBM: ', grid_search_lgb.best_estimator_.score(X_prep, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_num_leaves</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_min_data_in_leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.803485</td>\n",
       "      <td>128</td>\n",
       "      <td>0.3</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>64</td>\n",
       "      <td>0.3</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.800219</td>\n",
       "      <td>128</td>\n",
       "      <td>0.3</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.799966</td>\n",
       "      <td>64</td>\n",
       "      <td>0.3</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>0.796919</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.796465</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.792003</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score  mean_test_score param_num_leaves param_learning_rate  \\\n",
       "1                1         0.803485              128                 0.3   \n",
       "0                2         0.803030               64                 0.3   \n",
       "3                3         0.800219              128                 0.3   \n",
       "2                4         0.799966               64                 0.3   \n",
       "7                5         0.796919              128                   1   \n",
       "6                6         0.796465               64                   1   \n",
       "4                7         0.795000               64                   1   \n",
       "5                8         0.792003              128                   1   \n",
       "\n",
       "  param_min_data_in_leaf  \n",
       "1                    200  \n",
       "0                    200  \n",
       "3                    500  \n",
       "2                    500  \n",
       "7                    500  \n",
       "6                    500  \n",
       "4                    200  \n",
       "5                    200  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = ['rank_test_score','mean_test_score', 'param_num_leaves', 'param_learning_rate','param_min_data_in_leaf']\n",
    "pd.DataFrame(grid_search_lgb.cv_results_)[params].sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_cv = cross_val_predict(grid_search_lgb.best_estimator_, X_prep, y, cv=5)\n",
    "conf_mx = confusion_matrix(y, y_pred_cv)\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True) \n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "Best parameters based on CV:\n",
    "1. num_leaves: 128\n",
    "2. learning_rate: 0.3\n",
    "3. min_data_in_leaf: 200\n",
    "4. max_depth: 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep = transformation_pipeline.transform(test)\n",
    "y_pred = grid_search_lgb.best_estimator_.predict(test_prep)\n",
    "\n",
    "test_id = pd.read_csv('data/test.csv').id\n",
    "submission = pd.DataFrame({\"id\": test_id, \"status_group\": y_pred})\n",
    "submission = submission.replace({'status_group': num_to_class})\n",
    "submission.to_csv('submission_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='gini',\n",
       "                                                     max_depth=25,\n",
       "                                                     max_features='auto',\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=1,\n",
       "                                                     min_samples_split=2,\n",
       "                                                     min_weight_fraction_leaf=0.0,\n",
       "                                                     n_estimators=100,\n",
       "                                                     n_jobs=-1, oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0,...\n",
       "                                            colsample_bynode=1,\n",
       "                                            colsample_bytree=1, gamma=0,\n",
       "                                            learning_rate=0.1, max_delta_step=0,\n",
       "                                            max_depth=20, min_child_weight=1,\n",
       "                                            missing=None, n_estimators=100,\n",
       "                                            n_jobs=-1, nthread=None,\n",
       "                                            objective='binary:logistic',\n",
       "                                            random_state=0, reg_alpha=0,\n",
       "                                            reg_lambda=1, scale_pos_weight=1,\n",
       "                                            seed=None, silent=None, subsample=1,\n",
       "                                            verbosity=1))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meaningless_cols = ['id','waterpoint_type_group', 'source_type', 'source_class', \n",
    "                   'quantity_group', 'quality_group', 'payment_type', \n",
    "                   'management_group', 'extraction_type_group', 'extraction_type_class', \n",
    "                   'scheme_name', 'recorded_by', 'district_code', \n",
    "                   'region_code', 'region','subvillage', 'public_meeting']\n",
    "\n",
    "features_to_drop = ['latitude', 'longitude', 'date_recorded', 'num_private', 'permit', 'water_quality']\n",
    "\n",
    "transformation_pipeline = Pipeline([\n",
    "    ('meaningless_features', DropColumns(meaningless_cols)),\n",
    "    ('simple_imputer', OurSimpleImputer(permit=False)),\n",
    "    ('government', DataCorrection(installer=False, funder=False)),\n",
    "    ('geo_clusters', GeoClustering()),\n",
    "    ('advanced_imputer', OurAdvancedImputer(population_bucket=False)),\n",
    "    ('distance', Distance(strategy='eucledian')),\n",
    "    ('interactions', Interactions()),\n",
    "    ('other_features', OtherFeatures(num_private=False, dry_season=False)),\n",
    "    ('drop', DropColumns(features_to_drop)),\n",
    "    ('encoder', CountEncoder(handle_unknown=0))\n",
    "])\n",
    "\n",
    "class_to_num = {'functional': 2, 'non functional': 0, 'functional needs repair': 1}\n",
    "num_to_class = {0:'non functional', 1: 'functional needs repair', 2: 'functional'} \n",
    "\n",
    "X = original_train.drop('status_group', axis=1)\n",
    "y = original_train.status_group.replace(class_to_num)\n",
    "\n",
    "X_prep = transformation_pipeline.fit_transform(X)\n",
    "\n",
    "lgb_clf = LGBMClassifier(objective='multiclass',num_class=3, min_data_in_leaf=200, num_leaves=128, learning_rate=0.3, max_depth=16)\n",
    "xgb_clf = XGBClassifier(n_jobs=-1, max_depth=20, learning_rate=0.1)\n",
    "rf_clf = RandomForestClassifier(max_depth=25, n_estimators=100, n_jobs=-1)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', rf_clf), \n",
    "    ('lgb', lgb_clf),\n",
    "    ('xgb', xgb_clf)], voting='soft')\n",
    "\n",
    "voting_clf.fit(X_prep, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep = transformation_pipeline.transform(test)\n",
    "y_pred = voting_clf.predict(test_prep)\n",
    "\n",
    "test_id = pd.read_csv('data/test.csv').id\n",
    "submission = pd.DataFrame({\"id\": test_id, \"status_group\": y_pred})\n",
    "submission = submission.replace({'status_group': num_to_class})\n",
    "submission.to_csv('submission_voting_soft.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
